node-id-in-cluster = "node1"

http {
  listen-address {
    host: "0.0.0.0"
    port: 8080
  }
}

akka {
  loglevel = "debug"
  actor {
    provider: "akka.cluster.ClusterActorRefProvider"
    serializers {
      nasim = "serializer.ActorClusterSerializer"
    }
    serialization-bindings {
      "scalapb.GeneratedMessage" = nasim
    }
  }
  cluster {
    min-nr-of-members = 1
    seed-nodes = [
    ]
  }
  remote {
    netty.tcp {
      hostname = "127.0.0.1"
      port = 2552
    }
  }
}

akka.extensions = ["akka.cluster.client.ClusterClientReceptionist"]


redis {
  port: 6379
  port: ${?REDIS_PORT}

  host: "127.0.0.1"
  host: ${?REDIS_HOST}

  password: "foobared"
  password: ${?REDIS_PASSWORD}
}

rabbit9 {
  connection {
    hosts: ["127.0.0.1"]
    port: 5672
    username: "test"
    password: "test"
    connection-number: 1
    channel-per-connection: 10
    connection-retry-interval: 10 seconds
  }

  internal {
    default-exchange-name: "amq.direct"
    default-exchange-type: "direct"
    confirm-select: true // Enables publisher acknowledgements on channels
    routing-timeout: 5 seconds // Ask timeout for get your future value from rabbit9 router
    dispatcher: "rabbit9.default-dispatcher"
    subscription-retry-count: 20
    retry-min-backoff: 1 seconds
    retry-max-backoff: 5 seconds
    channel-message-timeout: 5 seconds

  }

  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-factor = 2.0
      parallelism-max = 4
    }
    throughput = 100
  }

}

services {
  kafka {
    producer {
      bootstrap.servers = "127.0.0.1:9092"
    }
    consumer {
      bootstrap.servers = "127.0.0.1:9092"
      commit-time-warning = 120
      wakeup-debug = false
      wakeup-timeout = 30s

    }
  }

  postgresql {
    db {
      host: "127.0.0.1"
      host: ${?POSTGRES_HOST}

      db: "test"
      db: ${?POSTGRES-DB}

      user: "test"
      user: ${?POSTGRES-USERNAME}

      password: "test"
      password: ${?POSTGRES_PASSWORD}
      port: 5432

      numThreads: 2
      maxConnections: 4
      connectionTimeout: 1000

      migration {
        test-mode: false
      }
    }
  }
}

akka-persistence-jdbc {
  slick {
    jndiDbName = "DefaultDatabase"
    profile = "slick.jdbc.PostgresProfile$"
  }
  shared-databases {
    slick {
      profile = "slick.jdbc.PostgresProfile$"
      db {
        host = "127.0.0.1"
        host: ${?POSTGRES_DB_HOST}
        url = "jdbc:postgresql://"${akka-persistence-jdbc.shared-databases.slick.db.host}":5432/test?reWriteBatchedInserts=true"
        user = "test"
        password = "test"
        driver = "org.postgresql.Driver"
        numThreads = 5
        maxConnections = 5
        minConnections = 1
      }

    }
  }
}


jdbc-journal {
  use-shared-db = "slick"
  slick {
    jndiDbName = "DefaultDatabase"
    profile = "slick.jdbc.PostgresProfile$"
  }
}

jdbc-snapshot-store {
  use-shared-db = "slick"
  slick {
    jndiDbName = "DefaultDatabase"
    profile = "slick.jdbc.PostgresProfile$"
  }
}

jdbc-read-journal {
  use-shared-db = "slick"
  slick {
    jndiDbName = "DefaultDatabase"
    profile = "slick.jdbc.PostgresProfile$"
  }
}


akka {
  actor {
    provider: "akka.cluster.ClusterActorRefProvider"
    serializers {
      nasim = "serializer.ActorClusterSerializer"
    }
    serialization-bindings {
      "scalapb.GeneratedMessage" = nasim
    }

    default-dispatcher {
      throughput = 10
    }
  }

  persistence {
    journal {
      plugin: "jdbc-journal"
    }
    snapshot-store {
      plugin: "jdbc-snapshot-store"
    }
  }

  log-config-on-start: false
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = on
  loglevel = "debug"

}

services.kafka.producer {
  bootstrap.servers = "127.0.0.1:9092"
  buffer-size = 1000

  # Tuning parameter of how many sends that can run in parallel.
  parallelism = 100

  # How long to wait for `KafkaProducer.close`
  close-timeout = 60s

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the producer stages. Some blocking may occur.
  # When this value is empty, the dispatcher configured for the stream
  # will be used.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Reconnection delay
    reconnect.backoff.ms: 5000
  }
}

services.kafka.consumer {
  bootstrap.servers = "127.0.0.1:9092"

  # Tuning property of scheduled polls.
  poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that blocking of the thread that
  # is executing the stage will be blocked.
  poll-timeout = 50ms

  # The stage will be await outstanding offset commit requests before
  # shutting down, but if that takes longer than this timeout it will
  # stop forcefully.
  stop-timeout = 30s

  # How long to wait for `KafkaConsumer.close`
  close-timeout = 20s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `TimeoutException`.
  commit-timeout = 15s

  # If the KafkaConsumer can't connect to the broker the poll will be
  # aborted after this timeout. The KafkaConsumerActor will throw
  # org.apache.kafka.common.errors.WakeupException which will be ignored
  # until max-wakeups limit gets exceeded.
  wakeup-timeout = 3s

  # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
  max-wakeups = 10

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Disable auto-commit by default
    enable.auto.commit = false
    # Reconnection delay
    reconnect.backoff.ms: 5000
  }
}

